# Setup and flow
general:
  model: nemotron4
  workload: nemotron4-15b
  workload_type: pretrain
  framework: nemo2

container:
  images: 
    - 'nvcr.io#nvidia/nemo:25.09.00'

downloads:
  hf_tokenizers:
    - 'nvidia/Nemotron-4-340B-Base'

repositories:
  nemo:
    url: "https://github.com/NVIDIA-NeMo/NeMo.git"
    commit: "032e4de5b7b69d2a53ca76397cf79ff0941951b1"
  megatron_core:
    url: "https://github.com/NVIDIA/Megatron-LM.git"
    commit: "20d73fe76eb03672c02f637e9b47e562b9010d4c"
  nemo_run:
    url: "https://github.com/NVIDIA-NeMo/Run.git"
    commit: "04f900a9c1cde79ce6beca6a175b4c62b99d7982"

setup:
  venv_req: true
  dependencies:
    pip:
      - package: nemo
        repo_key: nemo
        install_target: '.[nlp]'
      - 'scipy<1.13.0'
      - 'bitsandbytes==0.46.0'
      - 'nvidia-modelopt==0.35.1'
      - package: megatron-core
        repo_key: megatron_core
      - package: nemo_run
        repo_key: nemo_run

tools:
  cuda_cupti_lib: "13.0.85"

# Run
run:
  launcher_type: 'nemo'
  launch_script: 'launch.sh'

  gpu_configs:
    gb300:
      model_configs:
        - model_size: '15b'
          dtypes: ['fp8', 'bf16']
          scales: [16, 32, 64, 128, 256, 512]
    gb200:
      model_configs:
        - model_size: '15b'
          dtypes: ['fp8', 'bf16']
          scales: [16, 32, 64, 128, 256, 512]
    b200:
      model_configs:
        - model_size: '15b'
          dtypes: ['fp8', 'bf16']
          scales: [16, 32, 64, 128, 256, 512, 1024]
    h100:
      model_configs:
        - model_size: '15b'
          dtypes: ['fp8', 'bf16']
          scales: [16, 32, 64, 128, 256, 512, 1024, 2048]
