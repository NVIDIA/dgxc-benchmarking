# Setup
general:
  workload: llama3.1
  workload_type: pretrain
  gsw_version: '25.10'
  framework: megatron_bridge

container:
  images: 
    - 'nvcr.io#nvidia/nemo:25.09.00'

repositories:
  megatron_bridge:
    url: "https://github.com/NVIDIA-NeMo/Megatron-Bridge.git"
    commit: "f23d790cf6ff26f7731c637925656ed7f94473ba" 
  megatron_core:
    url: "https://github.com/NVIDIA/Megatron-LM.git"
    commit: "20d73fe76eb03672c02f637e9b47e562b9010d4c"
  nemo_run:
    url: "https://github.com/NVIDIA-NeMo/Run.git"
    commit: "04f900a9c1cde79ce6beca6a175b4c62b99d7982"

setup:
  venv_req: true
  dependencies:
    pip:
      - package: megatron_bridge
        repo_key: megatron_bridge
        install_target: '.'
      - package: megatron-core
        repo_key: megatron_core
      - package: nemo_run
        repo_key: nemo_run

tools:
  cuda_cupti_lib: "13.0.85"

# Run
run:
  launcher_type: 'megatron_bridge'
  launch_script: 'launch.sh'

  gpu_configs:
    gb300:
      model_configs:
        - model_size: '405b'
          dtypes:
            'fp8': [128, 256, 512, 1024]
        - model_size: '70b'
          dtypes:
            'fp8': [64,128,256,512,1024]
        - model_size: '8b'
          dtypes:
            'fp8': [8,16,32,64,128]
    gb200:
      model_configs:
        - model_size: '405b'
          dtypes: 
            'fp8': [128, 256, 512]
        - model_size: '70b'
          dtypes:
            'fp8': [64,128,256,512]
        - model_size: '8b'
          dtypes:
            'fp8': [8,16,32,64,128]
    b200:
      model_configs:
        - model_size: '405b'
          dtypes:
            'fp8': [128, 256, 512, 1024]
        - model_size: '70b'
          dtypes:
            'fp8': [64,128,256,512,1024]
        - model_size: '8b'
          dtypes:
            'fp8': [8,16,32,64,128]
    h100:
      model_configs:
        - model_size: '405b'
          dtypes: 
            'fp8': [1024]
        - model_size: '70b'
          dtypes:
            'fp8': [64,128,256,512,1024]
        - model_size: '8b'
          dtypes:
            'fp8': [8,16,32,64,128]
