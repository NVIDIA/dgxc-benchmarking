# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: MIT
#
# Permission is hereby granted, free of charge, to any person obtaining a
# copy of this software and associated documentation files (the "Software"),
# to deal in the Software without restriction, including without limitation
# the rights to use, copy, modify, merge, publish, distribute, sublicense,
# and/or sell copies of the Software, and to permit persons to whom the
# Software is furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
# DEALINGS IN THE SOFTWARE.

apiVersion: nvidia.com/v1alpha1
kind: DynamoGraphDeployment
metadata:
  name: gpt-oss-agg
spec:
  backendFramework: trtllm
  pvcs:
    - name: model-cache
      create: false
  services:
    Frontend:
      componentType: frontend
      dynamoNamespace: gpt-oss-agg
      extraPodSpec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: nvidia.com/dynamo-graph-deployment-name
                  operator: In
                  values:
                  - gpt-oss-agg-frontend
              topologyKey: kubernetes.io/hostname
        tolerations:
        - key: "dedicated"
          operator: "Equal"
          value: "user-workload"
          effect: "NoSchedule"
        - key: "dedicated"
          operator: "Equal"
          value: "user-workload"
          effect: "NoExecute"
        mainContainer:
          args:
          - python3 -m dynamo.frontend --router-mode round-robin --http-port 8000
          command:
          - /bin/sh
          - -c
          image: nvcr.io/nvidia/ai-dynamo/tensorrtllm-runtime:0.5.1-rc0.pre3
      replicas: 1
    TrtllmWorker:
      componentType: main
      dynamoNamespace: gpt-oss-agg
      envFromSecret: hf-token-secret
      volumeMounts:
        - name: model-cache
          mountPoint: /model-store
      sharedMemory:
        size: 80Gi
      extraPodSpec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: nvidia.com/gpu.present
                  operator: In
                  values:
                  - "true"
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: nvidia.com/dynamo-graph-deployment-name
                  operator: In
                  values:
                  - gpt-oss-agg
              topologyKey: kubernetes.io/hostname
        mainContainer:
          args:
          - |
            python3 -m dynamo.trtllm \
              --model-path "${MODEL_PATH}" \
              --served-model-name "openai/gpt-oss-120b" \
              --extra-engine-args "${ENGINE_ARGS}" \
              --tensor-parallel-size 4 \
              --expert-parallel-size 4 \
              --max-batch-size 800 \
              --free-gpu-memory-fraction 0.9
          command:
          - /bin/sh
          - -c
          image: nvcr.io/nvidia/ai-dynamo/tensorrtllm-runtime:0.5.1-rc0.pre3
          env:
          - name: TRTLLM_ENABLE_PDL
            value: "1"
          - name: TRT_LLM_DISABLE_LOAD_WEIGHTS_IN_PARALLEL
            value: "True"
          - name: SERVED_MODEL_NAME
            value: "openai/gpt-oss-120b"
          - name: ENGINE_ARGS
            value: "/opt/dynamo/configs/config.yaml"
          - name: MODEL_PATH
            value: "/model-store/hub/models--openai--gpt-oss-120b/snapshots/b5c939de8f754692c1647ca79fbf85e8c1e70f8a"
          - name: TLLM_LOG_LEVEL
            value: "INFO"
          volumeMounts:
          - mountPath: /opt/dynamo/configs
            name: llm-config
            readOnly: true
          workingDir: /workspace/components/backends/trtllm
        tolerations:
        - key: "dedicated"
          operator: "Equal"
          value: "user-workload"
          effect: "NoSchedule"
        - key: "dedicated"
          operator: "Equal"
          value: "user-workload"
          effect: "NoExecute"
        volumes:
        - configMap:
            name: llm-config
          name: llm-config
      replicas: 1
      resources:
        limits:
          gpu: "4"
        requests:
          gpu: "4"
