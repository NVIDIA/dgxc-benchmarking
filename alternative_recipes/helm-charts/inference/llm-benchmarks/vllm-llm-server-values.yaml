# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: MIT
#
# Permission is hereby granted, free of charge, to any person obtaining a
# copy of this software and associated documentation files (the "Software"),
# to deal in the Software without restriction, including without limitation
# the rights to use, copy, modify, merge, publish, distribute, sublicense,
# and/or sell copies of the Software, and to permit persons to whom the
# Software is furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
# DEALINGS IN THE SOFTWARE.

environment:
  # Server Configuration
  SERVER_TYPE: "vLLM"  # Can be "vLLM" or "NIM"
  SERVER_IMAGE: "vllm/vllm-openai:latest"  # Path to server image
  SERVER_ARGS: ""
  SERVER_PORT: "8000"  # The port the server will listen on
  SERVER_HEALTH_CHECK_ENDPOINT: "health"
  SERVER_ENDPOINT: "v1/chat/completions"  # The endpoint for chat completions
  
  # Model Configuration
  MODEL_NAME: "Qwen/Qwen3-0.6B"
  MODEL_NAME_CLEANED: "qwen3-0.6b"  # used in the benchmark result file name
  MODEL_TOKENIZER: "Qwen/Qwen3-0.6B"  # matches the path of the tokenizer on the HF registry
  HF_TOKEN: ""  # Your Hugging Face API token

  # Benchmark Parameters
  MIN_REQUESTS: "200"
  USE_CASES: "chat:128/128 chat:4096/512"
  CONCURRENCY_RANGE: "1 25 50 100"
  REQUEST_MULTIPLIER: "5"
  NUM_GPUS: "1"  # Configure the number of GPUs to use (e.g., "1", "2", "4", "8")
  # BENCHMARK_SERVER_IMAGE: "nvcr.io/nvidia/tritonserver:25.04-py3-sdk"  # Path to benchmark server image that contains genai-perf

  # Results Configuration
  RESULTS_PATH: "/vllm-llm-results"  # Directory where benchmark results will be stored

# Tolerations for pod scheduling on tainted nodes
# Tolerate the dedicated=user-workload taint (both NoSchedule and NoExecute effects)
tolerations:
  - key: "dedicated"
    operator: "Equal"
    value: "user-workload"
    effect: "NoSchedule"
  - key: "dedicated"
    operator: "Equal"
    value: "user-workload"
    effect: "NoExecute"

# provided on startup automatically by NVCF
ngcImagePullSecretName: ""  # Not needed for Docker Hub public images
ngcApiKeySecretName: ""  # vLLM doesn't need NGC API key
nvctNcaId: ""
nvctTaskId: ""
nvctTaskName: ""
nvctResultsDir: ""
nvctProgressFilePath: ""
