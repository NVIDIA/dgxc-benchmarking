## [v25.05.05] - 2025-10-22

### Changed
  - Llama3.1 Documentation - GA parameter fix

## [v25.05.04] - 2025-09-04

### Added
  - H100 512 GPU support for DeepSeek V3 Pretrain

### Changed
  - Bug fix for broken launch sequence due to transformers package update
  - Include latest LLMB tooling with improved capabilities for mass recipe submission and other updates

## [v25.05.03] - 2025-08-21

### Changed
  - Fixed nvcr.io links in DeepSeek V3 and Nemotron-H metadata files.

## [v25.05.02] - 2025-07-22

### Updated
  - H100 baseline performance numbers based on most recent runs
    - Grok1 314B

## [v25.05.01] - 2025-06-18

### Added
  - H100 support for the following recipes
    - Llama3.1 405B
    - Grok1 314B

### Changed
  - Fixed DeepSeek and Llama4 READMEs
  - Installer
    - Enforce min and max python versions
	- Use GRES properly for CPU_PARTITION jobs.
  - bitsandbytes update for ARM systems.

## [v25.05] - 2025-06-10

### Added
  - GB200 support for the following recipes
    - Llama3.1 405B
    - Grok1 314B
    - Nemotron4 15B/340B
    - Deepseek v3
    - Llama 4 Maverick
  - Nemotron-H 56B model training recipe for H100 GPUs
  - End to end installer and launcher for all recipes

### Changed
  - Recipes collection moved from [NGC Collection](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/dgxc-benchmarking/collections/dgxc-benchmarking) to [GitHub](https://github.com/NVIDIA/dgxc-benchmarking).

### Removed
  - NeMo GPT3 175b training recipe in favor of Nemotron4
  - Maxtext Llama 3 training recipe
  - Llama 3 SFT/LoRa fine tuning recipes


## [v25.04.01] - 2025-05-16

### Changed
  - Fixed MFU formula for Nemotron4 workload
  - Fixed setup script for FineTuning workload

## [v25.04] - 2025-04-30

### Changed
  - Llama3.1 and Nemotron4 benchmarks adopted 25.02.01 NeMo Framework and use NeMo2.0 interface
  - Llama3.1 and Nemotron4 benchmarks support checkpointing restart functionality


## [v25.03] - 2025-04-18

### Added
  - Deepseek R1 NIM inference benchmark

## [v25.02] - 2025-03-17

### Added
  - SFT/LoRA fine-tuning workload added based on 24.12 NeMo Framework
  - Maxtext Llama 3 70b added based on 25.01 Maxtext Framework
  - Llama 3 NIM inference benchmark
  - RAG pipeline blueprint benchmark

### Changed
  - Nemotron 15b updated to 24.12 NeMo Framework
  - Llama 3.1 8b, 70b, 405b updated to 24.12 NeMo Framework
  - Grok1 314b updated to 24.12 NeMo Framework

### Removed
  - HuggingFace Mistral fine-tuning
  - HuggingFace Llama fine-tuning
  - PaXML 175b training
  - Maxtext Llama 2 70b training

## [v25.01.1] - 2025-02-13

### Changed
  - Readme fixes
  - Fixed dataset generation on CSP clusters

## [v25.01] - 2025-02-11

### Changed
  - added profiling instructions and how to consume profile traces
  - improved setup scripts to fix sporadic enroot failures
  - updated README instructions to address user's feedback


## [v24.11] - 2024-12-13

### Added

  - Grok1 314b
  - Llama3.1 8b, 70b, 405b
  - Maxtext Llama2 70b
  - Nemotron 15b, 340b

### Changed

  - Updated launch scripts and READMEs to align recipes:
    - HuggingFace Mistral and Llama
    - Nemo Megatron
    - Paxml

### Removed

  - Llama2
  - Llama3

## [v24.05] - 2024-05-31

### Added

  - HuggingFace Mistral 7x8b fine tuning
  - HuggingFace Llama 70b LoRA fine tuning

### Changed

- Nemo Megatron 175B 
  - Update container version to 24.03.01
  - Simplified launch scripts with fewer dependencies due to removal of NeMo Launcher from the process
- Llama 
  - Update container version to 24.03.01
  - Simplified launch scripts with fewer dependencies due to removal of NeMo Launcher from the process
  - Add grad_sync_dtype to config

### Other

- Paxml
  - No change in container version, perf regression with the 2024-05-23 version

## [v24.04] - 2024-04-30

### Added

- Nemo Megatron 175B 24.01
- Llama 24.01
- Paxml 24.03.04
