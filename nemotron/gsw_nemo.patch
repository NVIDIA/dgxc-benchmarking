diff --git a/nemo/lightning/run/plugins.py b/nemo/lightning/run/plugins.py
index 664d8cd19..4c8a11eae 100644
--- a/nemo/lightning/run/plugins.py
+++ b/nemo/lightning/run/plugins.py
@@ -154,6 +154,7 @@ class NsysPlugin(run.Plugin):
     end_step: int
     ranks: Optional[list[int]] = None
     nsys_trace: Optional[list[str]] = None
+    nsys_extra_args: Optional[list[str]] = None
     gen_shape: bool = False
 
     def setup(self, task: run.Partial | run.Script, executor: run.Executor):
@@ -171,6 +172,8 @@ class NsysPlugin(run.Plugin):
         launcher = executor.get_launcher()
         launcher.nsys_profile = True
         launcher.nsys_trace = self.nsys_trace or ["nvtx", "cuda"]
+        if self.nsys_extra_args is not None:
+            launcher.nsys_extra_args = self.nsys_extra_args
 
 
 @dataclass(kw_only=True)
diff --git a/scripts/performance/argument_parser.py b/scripts/performance/argument_parser.py
index e60ed4b71..5ce9935be 100644
--- a/scripts/performance/argument_parser.py
+++ b/scripts/performance/argument_parser.py
@@ -80,7 +80,7 @@ def parse_cli_args():
     parser.add_argument(
         "-en",
         "--enable_nsys",
-        help="Enable Nsys profiling. Diabled by default",
+        help="Enable Nsys profiling. Disabled by default",
         action="store_true",
     )
     parser.add_argument(
@@ -258,6 +258,56 @@ def parse_cli_args():
         default=None,  # NOTE: DO NOT SET DEFAULT TO FALSE, IT WILL BE OVERRIDDEN BY THE RECOMMENDED MODEL CONFIGS
     )
 
+    parser.add_argument(
+        "-cpin",
+        "--cpu_pinning",
+        type=int,
+        help="Enable CPU pinning to improve performance on some clusters by setting numbers of CPUs per task. Disabled by default",
+        required=False,
+        # action="store_true",
+        default=0,  
+    )
+    parser.add_argument(
+        "-nlay",
+        "--num_layers",
+        type=int,
+        help="Sets number of model layers.",
+        required=False,
+        default=None,
+    )
+    parser.add_argument(
+        "-hs",
+        "--hidden_size",
+        type=int,
+        help="Sets hidden model size",
+        required=False,
+        default=None,
+    )
+    parser.add_argument(
+        "-pg",
+        "--profiling_gpu_metrics",
+        help="Enable CUDA graphs. Disabled by default",
+        action="store_true",
+        required=False,
+        default=False, 
+    )
+    parser.add_argument(
+        "-pss",
+        "--profiling_start_step",
+        type=int,
+        help="Defines start step for profiling",
+        required=False,
+        default=20
+    )
+    parser.add_argument(
+        "-pso",
+        "--profiling_stop_step",
+        type=int,
+        help="Defines start step for profiling",
+        required=False,
+        default=30
+    )
+
     def list_of_strings(arg):
         return arg.split(',')
 
diff --git a/scripts/performance/llm/pretrain_nemotron4_15b.py b/scripts/performance/llm/pretrain_nemotron4_15b.py
index b01b5cfd0..7ce5ce9e0 100644
--- a/scripts/performance/llm/pretrain_nemotron4_15b.py
+++ b/scripts/performance/llm/pretrain_nemotron4_15b.py
@@ -44,6 +44,8 @@ def override_recipe_configs(
     cp_size: int,
     vp_size: int,
     ep_size: int,
+    num_layers: int, 
+    hidden_size: int,
     enable_cuda_graphs: bool,
 ):
     """
@@ -68,6 +70,8 @@ def override_recipe_configs(
         cp_size,
         vp_size,
         ep_size,
+        num_layers,
+        hidden_size
     )
     gpu_type = args.gpu.lower()
 
@@ -92,7 +96,9 @@ def override_recipe_configs(
         recipe.trainer.callbacks[comm_overlap_callback_idx].tp_comm_overlap_cfg = tp_comm_overlap_cfg
 
     recipe.model.config.enable_cuda_graph = enable_cuda_graphs
+
     recipe.trainer.strategy.use_te_rng_tracker = enable_cuda_graphs
+    recipe.trainer.limit_val_batches = 0
 
     return recipe
 
@@ -102,14 +108,27 @@ if __name__ == "__main__":
     args_sanity_check(args)
 
     kwargs = get_user_configs(args.gpu.lower(), "pre_train", "nemotron4", "15b", args)
-    num_nodes, mbs, gbs, tp_size, pp_size, cp_size, vp_size, ep_size, _, enable_cuda_graphs = kwargs
+    num_nodes, mbs, gbs, tp_size, pp_size, cp_size, vp_size, ep_size, num_layers, hidden_size, _, enable_cuda_graphs = kwargs
 
     recipe = override_recipe_configs(
-        args, num_nodes, mbs, gbs, tp_size, pp_size, cp_size, vp_size, ep_size, enable_cuda_graphs
+        args, num_nodes, mbs, gbs, tp_size, pp_size, cp_size, vp_size, ep_size, num_layers, hidden_size, enable_cuda_graphs
     )
 
-    exp_config = f"{num_nodes}nodes_tp{tp_size}_pp{pp_size}_cp{cp_size}_vp{vp_size}_{mbs}mbs_{gbs}gbs"
-    exp_name = f"{splitext(basename(__file__))[0]}_{args.compute_dtype}_{exp_config}"
+    pinning_args=[]
+    exp_tuning = ""
+    if args.cpu_pinning > 0:
+        pinning_args = [
+            "--cpu-bind=verbose", 
+            f"--cpus-per-task={args.cpu_pinning}", 
+            "--hint=multithread", 
+            "--distribution=*:block"
+        ]
+        exp_tuning += "_pinned"
+
+    if args.enable_nsys:
+        exp_tuning += "_nsys"
+
+    exp_name = f"{splitext(basename(__file__))[0]}_{args.compute_dtype}_{args.num_gpus}{exp_tuning}"
 
     executor = slurm_executor(
         args.account,
@@ -121,6 +140,7 @@ if __name__ == "__main__":
         args.container_image,
         custom_mounts=args.custom_mounts,
         custom_env_vars={},
+        custom_srun_args=pinning_args,
         hf_token=args.hf_token,
         nemo_home=args.nemo_home,
         wandb_key=args.wandb_key,
@@ -128,7 +148,19 @@ if __name__ == "__main__":
 
     plugins = [PerfEnvPlugin(enable_vboost=True, nccl_pp_comm_chunksize=2097152 if pp_size > 1 else None)]
     if args.enable_nsys:
-        plugins.append(NsysPlugin(start_step=5, end_step=6))
+        nsys_ranks=[0,1,2,3,4,5,6,7]
+        nsys_args=[
+            "--force-overwrite=true",
+            "--capture-range=cudaProfilerApi",
+            "--capture-range-end=stop",
+            "--cuda-graph-trace=node",
+            "--cuda-event-trace=false"]
+        
+        if args.profiling_gpu_metrics:
+            nsys_ranks=[0]
+            nsys_args.append("--gpu-metrics-device=all")
+        
+        plugins.append(NsysPlugin(start_step=args.profiling_start_step, end_step=args.profiling_stop_step, ranks=nsys_ranks, nsys_extra_args=nsys_args))
 
     with run.Experiment(exp_name) as exp:
         exp.add(
diff --git a/scripts/performance/llm/pretrain_nemotron4_340b.py b/scripts/performance/llm/pretrain_nemotron4_340b.py
index a6cc46b5e..f9466ded6 100644
--- a/scripts/performance/llm/pretrain_nemotron4_340b.py
+++ b/scripts/performance/llm/pretrain_nemotron4_340b.py
@@ -46,6 +46,8 @@ def override_recipe_configs(
     cp_size: int,
     vp_size: int,
     ep_size: int,
+    num_layers: int, 
+    hidden_size: int,
     enable_cuda_graphs: bool,
 ):
     """
@@ -70,6 +72,8 @@ def override_recipe_configs(
         cp_size,
         vp_size,
         ep_size,
+        num_layers,
+        hidden_size
     )
     gpu_type = args.gpu.lower()
 
@@ -96,7 +100,9 @@ def override_recipe_configs(
         recipe.trainer.callbacks[comm_overlap_callback_idx].tp_comm_overlap_cfg = tp_comm_overlap_cfg
 
     recipe.model.config.enable_cuda_graph = enable_cuda_graphs
+
     recipe.trainer.strategy.use_te_rng_tracker = enable_cuda_graphs
+    recipe.trainer.limit_val_batches = 0
 
     return recipe
 
@@ -106,14 +112,27 @@ if __name__ == "__main__":
     args_sanity_check(args)
 
     kwargs = get_user_configs(args.gpu.lower(), "pre_train", "nemotron4", "340b", args)
-    num_nodes, mbs, gbs, tp_size, pp_size, cp_size, vp_size, ep_size, _, enable_cuda_graphs = kwargs
+    num_nodes, mbs, gbs, tp_size, pp_size, cp_size, vp_size, ep_size, num_layers, hidden_size, _, enable_cuda_graphs = kwargs
 
     recipe = override_recipe_configs(
-        args, num_nodes, mbs, gbs, tp_size, pp_size, cp_size, vp_size, ep_size, enable_cuda_graphs
+        args, num_nodes, mbs, gbs, tp_size, pp_size, cp_size, vp_size, ep_size, num_layers, hidden_size, enable_cuda_graphs
     )
 
-    exp_config = f"{num_nodes}nodes_tp{tp_size}_pp{pp_size}_cp{cp_size}_vp{vp_size}_{mbs}mbs_{gbs}gbs"
-    exp_name = f"{splitext(basename(__file__))[0]}_{args.compute_dtype}_{exp_config}"
+    pinning_args=[]
+    exp_tuning = ""
+    if args.cpu_pinning > 0:
+        pinning_args = [
+            "--cpu-bind=verbose", 
+            f"--cpus-per-task={args.cpu_pinning}", 
+            "--hint=multithread", 
+            "--distribution=*:block"
+        ]
+        exp_tuning += "_pinned"
+
+    if args.enable_nsys:
+        exp_tuning += "_nsys"
+
+    exp_name = f"{splitext(basename(__file__))[0]}_{args.compute_dtype}_{args.num_gpus}{exp_tuning}"
 
     executor = slurm_executor(
         args.account,
@@ -125,6 +144,7 @@ if __name__ == "__main__":
         args.container_image,
         custom_mounts=args.custom_mounts,
         custom_env_vars={},
+        custom_srun_args=pinning_args,
         hf_token=args.hf_token,
         nemo_home=args.nemo_home,
         wandb_key=args.wandb_key,
@@ -132,7 +152,19 @@ if __name__ == "__main__":
 
     plugins = [PerfEnvPlugin(enable_vboost=True, nccl_pp_comm_chunksize=2097152 if pp_size > 1 else None)]
     if args.enable_nsys:
-        plugins.append(NsysPlugin(start_step=5, end_step=6))
+        nsys_ranks=[0,1,2,3,4,5,6,7]
+        nsys_args=[
+            "--force-overwrite=true",
+            "--capture-range=cudaProfilerApi",
+            "--capture-range-end=stop",
+            "--cuda-graph-trace=node",
+            "--cuda-event-trace=false"]
+        
+        if args.profiling_gpu_metrics:
+            nsys_ranks=[0]
+            nsys_args.append("--gpu-metrics-device=all")
+        
+        plugins.append(NsysPlugin(start_step=args.profiling_start_step, end_step=args.profiling_stop_step, ranks=nsys_ranks, nsys_extra_args=nsys_args))
 
     with run.Experiment(exp_name) as exp:
         exp.add(
diff --git a/scripts/performance/utils.py b/scripts/performance/utils.py
index 49d473bf6..23de7cf6d 100644
--- a/scripts/performance/utils.py
+++ b/scripts/performance/utils.py
@@ -19,6 +19,8 @@ from typing import Dict, List, Optional
 
 import nemo_run as run
 import pandas as pd
+import nemo.lightning as nl
+from nemo.lightning import AutoResume
 from lightning.pytorch.callbacks.callback import Callback
 from nemo_run.config import get_nemorun_home
 from numpy import nan
@@ -77,6 +79,10 @@ def slurm_executor(
     if nemo_home != DEFAULT_NEMO_CACHE_HOME:  # DO NOT change this to 'DEFAULT_NEMO_HOME'/'NEMO_HOME'
         env_vars.update({"NEMO_HOME": nemo_home})
         mounts.extend([f"{nemo_home}:{nemo_home}"])
+
+    #Extra location mount for checkpointing support
+    STAGE_PATH = os.getenv('STAGE_PATH')
+    mounts.extend([f"{STAGE_PATH}:{STAGE_PATH}"])
     if hf_token is not None:
         env_vars.update({"HF_TOKEN": hf_token, "TRANSFORMERS_OFFLINE": "0"})
 
@@ -99,7 +105,7 @@ def slurm_executor(
         time=time_limit,
         mem="0",
         exclusive=True,
-        packager=run.GitArchivePackager(),
+        packager=run.Packager(),
     )
 
     return executor
@@ -179,7 +185,10 @@ def get_user_configs(gpu: str, task: str, model_name: str, model_size: str, args
     enable_cuda_graphs = config.get("cuda_graphs") if args.cuda_graphs is None else args.cuda_graphs
     enable_cuda_graphs = False if enable_cuda_graphs is None else bool(int(enable_cuda_graphs))
 
-    kwargs = num_nodes, mbs, gbs, tp_size, pp_size, cp_size, vp_size, ep_size, etp_size
+    num_layers = args.num_layers
+    hidden_size = args.hidden_size
+
+    kwargs = num_nodes, mbs, gbs, tp_size, pp_size, cp_size, vp_size, ep_size, num_layers, hidden_size, etp_size
     kwargs = [int(arg) if arg is not None else arg for arg in kwargs] + [enable_cuda_graphs]
 
     return kwargs
@@ -201,6 +210,8 @@ def set_primary_perf_configs(
     cp_size: int,
     vp_size: int,
     ep_size: int,
+    num_layers: int,
+    hidden_size: int,
     etp_size: Optional[int] = None,
 ):
     """Set experiment configs we usually tune for performance of all models."""
@@ -223,6 +234,10 @@ def set_primary_perf_configs(
 
     recipe.trainer.strategy.sequence_parallel = bool(tp_size > 1)
 
+    # other parameters to make them explicit in yaml configs
+    recipe.model.config.num_layers = num_layers
+    recipe.model.config.hidden_size = hidden_size
+
     # callback configs
     comm_overlap_callback_idx = get_comm_overlap_callback_idx(recipe.trainer.callbacks)
     dp_size = (num_nodes * num_gpus_per_node) / (tp_size * pp_size * cp_size)
@@ -245,9 +260,27 @@ def set_primary_perf_configs(
 
     # Misc. for overall faster experiment runtime
     recipe.log.ckpt = None
-    recipe.trainer.enable_checkpointing = False
+    recipe.trainer.enable_checkpointing = (os.getenv('ENABLE_CHECKPOINT', 'false') == 'true')
     recipe.trainer.val_check_interval = max_steps
     recipe.trainer.log_every_n_steps = 1
+    load_checkpoint_path = os.getenv('LOAD_CHECKPOINT_PATH')
+
+    if recipe.trainer.enable_checkpointing or load_checkpoint_path is not None:
+        recipe.trainer.callbacks[comm_overlap_callback_idx].overlap_param_gather_with_optimizer_step = False
+
+    if load_checkpoint_path is not None:
+        recipe.resume = run.Config(
+            AutoResume,
+            resume_if_exists=True,
+            resume_ignore_no_checkpoint=False,
+            restore_config=run.Config(
+                nl.RestoreConfig,
+                path=load_checkpoint_path,
+                load_model_state=True,
+                load_optim_state=True,
+                load_artifacts=False,
+            ),
+        )
 
     return recipe
 
